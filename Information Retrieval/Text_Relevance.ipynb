{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основано на статье: https://bit.ly/3aTPrHN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/twinkle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import pyaspeller\n",
    "from pymystem3 import Mystem\n",
    "from langdetect import detect\n",
    "\n",
    "from string import punctuation\n",
    "from PyDictionary import PyDictionary\n",
    "from rank_bm25 import BM25Plus\n",
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "\n",
    "import googletrans\n",
    "#from translate import Translator\n",
    "from yandex.Translater import Translater\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import platform\n",
    "from itertools import combinations\n",
    "import multiprocessing\n",
    "from io import StringIO\n",
    "from multiprocessing import freeze_support\n",
    "\n",
    "from lxml import etree\n",
    "from html.parser import HTMLParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOCUMENTS = 38114\n",
    "MEAN_DOCLEN = 12155\n",
    "TOTAL_LEMMS = N_DOCUMENTS * MEAN_DOCLEN\n",
    "\n",
    "K1 = 1\n",
    "K2 = 1 / 800\n",
    "\n",
    "W_PAIR = 0.3\n",
    "\n",
    "W_P = 2\n",
    "W_I = 0.7\n",
    "W_S = 0.5\n",
    "W_W = 0.3\n",
    "\n",
    "W_ALL = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_path = './data/queries.numerate.txt'\n",
    "queries_processed_path = './data/queries.numerate_processed_stem.txt'\n",
    "\n",
    "change_lang = ('da', 'sq', 'cy')\n",
    "change_rule = {\n",
    "    'q': 'й', 'w': 'ц', 'e': 'у', 'r': 'к', 't': 'е', 'y': 'н', 'u': 'г', 'i': 'ш',\n",
    "    'o': 'щ', 'p': 'з', '[': 'х', ']': 'ъ', 'a': 'ф', 's': 'ы', 'd': 'в', 'f': 'а',\n",
    "    'g': 'п', 'h': 'р', 'j': 'о', 'k': 'л', 'l': 'д', ';': 'ж', '\\'': 'э', 'z': 'я',\n",
    "    'x': 'ч', 'c': 'с', 'v': 'м', 'b': 'и', 'n': 'т', 'm': 'ь', ',': 'б', '.': 'ю'\n",
    "    }\n",
    "\n",
    "digits = set('1234567890')\n",
    "en_symbols = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "ru_symbols = set('йцукенгшщзхъэждлорпавыфячсмитьбюЙЦУКЕНГШЩЗХЪЭЖДЛОРПАВЫФЯЧСМИТЬБЮ')\n",
    "\n",
    "stemmer = Mystem()\n",
    "tr = Translater()\n",
    "tr.set_key('CENSORED')\n",
    "\n",
    "speller = pyaspeller.YandexSpeller(find_repeat_words=True, ignore_capitalization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "мультивиз в израиль какой страна можно посещать\n",
      "\n",
      "мультивиз в израиль как страна можно посещать\n",
      "\n",
      "---- ---- ----\n",
      "2\n",
      "надо ли носить компресс гольф после операция на голеностоп\n",
      "надо ли носить компрес гольф после операция на голеностоп\n",
      "нужно ли носить компресс в гольф после операция на лодыжка\n",
      "нужно ли я носить оборачивать гольф после операция на лодыжка\n",
      "---- ---- ----\n",
      "3\n",
      "жировик на спина можно ли применять пиявка\n",
      "\n",
      "шишка на спина можно ли применять пиявка\n",
      "\n",
      "---- ---- ----\n",
      "4\n",
      "как прописывать просто админ\n",
      "как прописывать просто адмика\n",
      "как зарегистрировать администратор\n",
      "как зарегистрировать админ\n",
      "---- ---- ----\n",
      "5\n",
      "хто буде судить суперкубок 2017 м ж шахтер динамо кий в дбудеться в одес \n",
      "\n",
      "хто буде судити суперкубок 2017 м ж шахтар динамо якия в дбудеться в одес \n",
      "\n",
      "кто быть судить суперкубок 2017 шахтер динамо кий в budelse в одесса\n",
      "кто быть судить суперкубок 2017 шахтер динамо акиа в budelse в одесса\n",
      "---- ---- ----\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def process_query(query):\n",
    "    if len(query) == 0:\n",
    "        return query\n",
    "\n",
    "    if detect(query) in change_lang:\n",
    "        query = ''.join(change_rule[c] if c in change_rule.keys() else c for c in query)\n",
    "\n",
    "    query = ''.join([c if c in en_symbols or c in ru_symbols or c in digits else ' ' for c in query])\n",
    "    words = [word.lower().strip() for word in query.split(' ')]\n",
    "    query = ' '.join(words)\n",
    "    lemmas = stemmer.lemmatize(query)\n",
    "    query = ' '.join(word for word in lemmas if len(word) > 0 and word != '\\n')\n",
    "    query = re.sub(' +', ' ', query)\n",
    "    return query\n",
    "\n",
    "with open(queries_path, 'r', encoding='UTF-8') as queries_file, \\\n",
    "        open(queries_processed_path, 'w', encoding='UTF-8') as queries_processed_file:\n",
    "\n",
    "    for line in queries_file:\n",
    "        idx, query = (part.strip() for part in line.split('\\t'))\n",
    "        query = process_query(query)\n",
    "        correction = speller.spell(query) \n",
    "\n",
    "        if len(list(correction)) > 0:\n",
    "            query_ex = query\n",
    "            any_corrections = False\n",
    "            for w in speller.spell(query):\n",
    "                if len(w['s']) > 0:\n",
    "                    any_corrections = True\n",
    "                    query = query.replace(w['word'], w['s'][0])\n",
    "            query_main = process_query(query)\n",
    "            if any_corrections == False:\n",
    "                query_ex = ''\n",
    "        else:\n",
    "            query_ex = ''\n",
    "            query_main = query\n",
    "\n",
    "        tr.set_text(query_main)\n",
    "        tr.set_from_lang(tr.detect_lang())\n",
    "        tr.set_to_lang('en')\n",
    "        trans = tr.set_text(tr.translate())\n",
    "        tr.set_from_lang('en')\n",
    "        tr.set_to_lang('ru')\n",
    "        query_trans_main = process_query(tr.translate())\n",
    "\n",
    "        if query_ex:\n",
    "            tr.set_text(query_ex)\n",
    "            tr.set_from_lang(tr.detect_lang())\n",
    "            tr.set_to_lang('en')\n",
    "            trans = tr.set_text(tr.translate())\n",
    "            tr.set_from_lang('en')\n",
    "            tr.set_to_lang('ru')\n",
    "            query_trans_ex = process_query(tr.translate())\n",
    "        else:\n",
    "            query_trans_ex = ''\n",
    "\n",
    "        print(idx)\n",
    "        print(query_main)\n",
    "        print(query_ex)\n",
    "        print(query_trans_main)\n",
    "        print(query_trans_ex)\n",
    "        print('---- ---- ----')\n",
    "\n",
    "        queries_processed_file.write(idx + '\\t' + query_main.strip() + '\\t' + \\\n",
    "                                     query_ex.strip() + '\\t' + query_trans_main.strip() + '\\t' + query_trans_ex.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_stem_files(base_path):\n",
    "    files = []\n",
    "    text_dir_paths = [f for f in os.listdir(base_path) if f.count('_') == 3 and f.find('text_stem') >= 0]\n",
    "    for dir_path in text_dir_paths:\n",
    "        for file_path in os.listdir(base_path + dir_path):\n",
    "            yield base_path + dir_path + '/' + file_path\n",
    "\n",
    "\n",
    "def get_title_stem_files(base_path):\n",
    "    files = []\n",
    "    text_dir_paths = [f for f in os.listdir(base_path) if f.count('_') == 3 and f.find('title_stem') >= 0]\n",
    "    for dir_path in text_dir_paths:\n",
    "        for file_path in os.listdir(base_path + dir_path):\n",
    "            yield base_path + dir_path + '/' + file_path\n",
    "\n",
    "\n",
    "def get_base_file_path(path):\n",
    "    if path.find('text'):\n",
    "        path = path.replace('_text', '')\n",
    "    if path.find('title'):\n",
    "        path = path.replace('_title', '')\n",
    "    if path.find('stem'):\n",
    "        path = path.replace('_stem', '')\n",
    "    return path\n",
    "\n",
    "\n",
    "def file_path_to_tf_path(path):\n",
    "    return path.replace('content', 'statistics/tf').replace('.dat', '.pkz')\n",
    "\n",
    "\n",
    "def file_path_to_pairs_path(path):\n",
    "    return path.replace('content', 'statistics/pairs').replace('.dat', '.pkz')\n",
    "\n",
    "\n",
    "def file_path_to_hop_pairs_path(path):\n",
    "    return path.replace('content', 'statistics/hop_pairs').replace('.dat', '.pkz')\n",
    "\n",
    "def file_path_to_hop_hop_pairs_path(path):\n",
    "    return path.replace('content', 'statistics/hop_hop_pairs').replace('.dat', '.pkz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113\n",
      "38113"
     ]
    }
   ],
   "source": [
    "base_path = './data/content/'\n",
    "url_path = './data/urls.numerate.txt'\n",
    "num_path = './data/path.numerate.txt'\n",
    "num_sort_path = './data/path.numerate.sort.txt'\n",
    "p_n_path = './data/path.numerate.pkz'\n",
    "n_p_path = './data/numerate.path.pkz'\n",
    "\n",
    "path_numerate = dict()\n",
    "numerate_path = dict()\n",
    "url_to_num = dict()\n",
    "\n",
    "dir_paths = [f for f in os.listdir(base_path) if f.count('_') == 1]\n",
    "\n",
    "with open(url_path, 'r', encoding='UTF-8', errors='ignore') as url_file:\n",
    "    for line in url_file:\n",
    "        num, url = line.strip().split('\\t')\n",
    "        url_to_num[url] = num\n",
    "\n",
    "cnt = 0\n",
    "with open(num_path, 'a', encoding='UTF-8', errors='ignore') as num_file:\n",
    "    for dir_path in dir_paths:\n",
    "        file_paths = [f for f in os.listdir(base_path + dir_path)]\n",
    "        for file_path in file_paths:\n",
    "            full_file_path = base_path + dir_path + '/' + file_path\n",
    "            with open(full_file_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "                url = file.readline().strip()\n",
    "                num_file.write(full_file_path + '\\t' + str(url_to_num[url]) + '\\n')\n",
    "                print('\\r{}'.format(cnt), end='')\n",
    "                cnt += 1\n",
    "\n",
    "cnt = 0\n",
    "with open(num_path, 'r', encoding='UTF-8', errors='ignore') as num_file, \\\n",
    "    open(num_sort_path, 'w', encoding='UTF-8', errors='ignore') as num_sort_file:\n",
    "    paths = defaultdict(str)\n",
    "    for line in num_file:\n",
    "        path, idx = (part.strip() for part in line.split('\\t'))\n",
    "        paths[idx] = path\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "    for i in range(1, int(N_DOCUMENTS) + 1):\n",
    "        num_sort_file.write(paths[str(i)] + '\\t' + str(i) + '\\n')\n",
    "\n",
    "print()        \n",
    "\n",
    "cnt = 0\n",
    "with open(num_path, 'r', encoding='UTF-8', errors='ignore') as num_file, \\\n",
    "        open(p_n_path, 'wb') as p_n_file, open(n_p_path, 'wb') as n_p_file:\n",
    "    for line in num_file:\n",
    "        path, num = line.strip().split('\\t')\n",
    "        path_numerate[path] = int(num)\n",
    "        numerate_path[int(num)] = path\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "    pickle.dump(path_numerate, p_n_file)\n",
    "    pickle.dump(numerate_path, n_p_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_path = './data/queries.numerate_processed_stem.txt'\n",
    "base_path = './data/queries.numerate_processed_chunks/'\n",
    "n_splits = 4\n",
    "split_size = 100\n",
    "\n",
    "split_files = [open(base_path + 'chunk_{}'.format(str(idx)) + '.txt',\n",
    "                    'w',\n",
    "                    encoding='UTF-8',\n",
    "                    errors='ignore')\n",
    "               for idx in range(1, n_splits + 1)]\n",
    "\n",
    "with open(queries_path, 'r', encoding='UTF-8', errors='ignore') as queries_file:\n",
    "    for line in queries_file:\n",
    "        idx, query_main, query_ex, query_trans_main, query_trans_ex = (part.strip() for part in line.split('\\t'))\n",
    "        split_files[int(idx) // split_size].write(idx + '\\t' + query_main.strip() + '\\t' + query_ex.strip() + '\\t' + query_trans_main.strip() + '\\t' + query_trans_ex.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._text = []\n",
    "        self._title = []\n",
    "        self._in_title = False\n",
    "\n",
    "    def handle_data(self, text):\n",
    "        text = text.strip()\n",
    "        if len(text) > 0:\n",
    "            if self.accept_field(text) or self._in_title:\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "            if self._in_title:\n",
    "                self._title.append(text)\n",
    "            else:\n",
    "                self._text.append(text)\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'title':\n",
    "            self._in_title = True\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'title':\n",
    "            self._in_title = False\n",
    "\n",
    "    def text(self):\n",
    "        text = ' '.join(self._text)\n",
    "        # only literals or digits\n",
    "        text = re.sub('\\W+', ' ', text)\n",
    "        # split number from text, make lower\n",
    "        text = ' '.join(re.split('(\\d+)', text)).lower()\n",
    "        text = ' '.join(text.split(' '))\n",
    "        return text\n",
    "\n",
    "    def title(self):\n",
    "        text = ' '.join(self._title)\n",
    "        # only literals or digits\n",
    "        text = re.sub('\\W+', ' ', text)\n",
    "        # split number from text, make lower\n",
    "        text = ' '.join(re.split('(\\d+)', text)).lower()\n",
    "        text = ' '.join(text.split(' '))\n",
    "        return text\n",
    "\n",
    "    def accept_field(self, field):\n",
    "        en_cnt = 0\n",
    "        ru_cnt = 0\n",
    "        di_cnt = 0\n",
    "        for c in field:\n",
    "            if c in en_symbols:\n",
    "                en_cnt += 1\n",
    "            elif c in ru_symbols:\n",
    "                ru_cnt += 1\n",
    "            elif c in digits:\n",
    "                di_cnt += 1\n",
    "        if en_cnt + ru_cnt == 0:\n",
    "            return False\n",
    "        en_proba = en_cnt / (en_cnt + ru_cnt)\n",
    "        if en_proba < 0.95 or en_cnt == len(field) - ru_cnt - field.count(' '):\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_title(path, *args):\n",
    "    with open(path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # fix DOM tree using lxml tools\n",
    "        parser = etree.HTMLParser()\n",
    "        tree = etree.parse(StringIO(text), parser)\n",
    "        text = str(etree.tostring(tree.getroot(), pretty_print=True, method=\"html\"), encoding='UTF-8')\n",
    "\n",
    "        # parse fixed html and get title, text, meta, headers, ...\n",
    "        parser = TextHTMLParser()\n",
    "        parser.feed(text)\n",
    "        return parser.text(), parser.title()\n",
    "\n",
    "def get_text_and_title_stem(text_path, title_path):\n",
    "    with open(text_path, 'r', encoding='UTF-8', errors='ignore') as text_file, \\\n",
    "            open(title_path, 'r', encoding='UTF-8', errors='ignore') as title_file:\n",
    "        text = text_file.read()\n",
    "        title = title_file.read()\n",
    "\n",
    "    text_words, title_words = stemmer.lemmatize(text), stemmer.lemmatize(title)\n",
    "\n",
    "    text = ' '.join(word for word in text_words if len(word) > 0 and word != '\\n')\n",
    "    title = ' '.join(word for word in title_words if len(word) > 0 and word != '\\n')\n",
    "    text = ' '.join(text.split(' '))\n",
    "    title = ' '.join(title.split(' '))\n",
    "    return text, title\n",
    "\n",
    "\n",
    "def prepare_dir(base_path, step_suffix, dir_path, preparer):\n",
    "    cnt = 0\n",
    "    # creating directories\n",
    "    try:\n",
    "        os.mkdir(base_path + dir_path + step_suffix[0])\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(base_path + dir_path.replace('text', 'title') + step_suffix[1])\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    file_paths = [f for f in os.listdir(base_path + dir_path)]\n",
    "    for file_path in file_paths:\n",
    "        real_path = base_path + dir_path + '/' + file_path\n",
    "        text_real_path_result = base_path + dir_path + step_suffix[0] + '/' + file_path\n",
    "        title_real_path_result = base_path + dir_path.replace('text', 'title') + step_suffix[1] + '/' + file_path\n",
    "\n",
    "        if os.path.exists(text_real_path_result) and os.path.exists(title_real_path_result):\n",
    "            continue\n",
    "\n",
    "        text, title = preparer(real_path, real_path.replace('text', 'title'))\n",
    "\n",
    "        with open(text_real_path_result, 'w', encoding='UTF-8', errors='ignore') as result_file:\n",
    "            result_file.write(text)\n",
    "        with open(title_real_path_result, 'w', encoding='UTF-8', errors='ignore') as result_file:\n",
    "            result_file.write(title)\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113\n"
     ]
    }
   ],
   "source": [
    "base_path = './data/content/'\n",
    "dir_paths = [f for f in os.listdir(base_path) if f.count('_') == 1]\n",
    "step_suffix = ['_text', '_title']\n",
    "\n",
    "for dir_path in dir_paths:\n",
    "    # get text and title for each document\n",
    "    prepare_dir(base_path, step_suffix, dir_path, get_text_and_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113\n"
     ]
    }
   ],
   "source": [
    "base_path = './data/content/'\n",
    "dir_paths = [f for f in os.listdir(base_path) if f.count('_') == 2 and f.find('text') >= 0]\n",
    "step_suffix = ['_stem', '_stem']\n",
    "\n",
    "for dir_path in dir_paths:\n",
    "    # get stemmed text and title fot each document\n",
    "    prepare_dir(base_path, step_suffix, dir_path, get_text_and_title_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113"
     ]
    }
   ],
   "source": [
    "def count_tf(file_generator):\n",
    "    cnt = 0\n",
    "    for file_path in file_generator:\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        tf_dict = defaultdict(int)\n",
    "        with open(file_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            text = file.read().strip()\n",
    "            words = text.split(' ')\n",
    "            for word in words:\n",
    "                tf_dict[word] += 1\n",
    "\n",
    "        tf_path = file_path_to_tf_path(file_path)\n",
    "        os.makedirs(os.path.dirname(tf_path), exist_ok=True)\n",
    "        with open(tf_path, 'wb') as tf_file:\n",
    "            pickle.dump(tf_dict, tf_file)\n",
    "\n",
    "base_path = './data/content/'\n",
    "base_result_path = './data/statistics/'\n",
    "\n",
    "count_tf(get_text_stem_files(base_path))\n",
    "count_tf(get_title_stem_files(base_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113"
     ]
    }
   ],
   "source": [
    "def count_df(file_generator, df_path):\n",
    "    df_dict = defaultdict(int)\n",
    "    cnt = 0\n",
    "    for file_path in file_generator:\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        tmp_dict = defaultdict(int)\n",
    "        with open(file_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            text = file.read().strip()\n",
    "            words = text.split(' ')\n",
    "            for word in words:\n",
    "                tmp_dict[word] = 1\n",
    "        for key in tmp_dict.keys():\n",
    "            df_dict[key] += 1\n",
    "    with open(df_path, 'wb') as df_file:\n",
    "        pickle.dump(df_dict, df_file)\n",
    "\n",
    "\n",
    "base_path = './data/content/'\n",
    "base_result_path = './data/statistics/'\n",
    "\n",
    "text_df_path = base_result_path + 'text_df_count.pkz'\n",
    "title_df_path = base_result_path + 'title_df_count.pkz'\n",
    "\n",
    "count_df(get_text_stem_files(base_path), text_df_path)\n",
    "count_df(get_title_stem_files(base_path), title_df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113"
     ]
    }
   ],
   "source": [
    "def count_cf(file_generator, cf_path):\n",
    "    cf_dict = defaultdict(int)\n",
    "    cnt = 0\n",
    "    for file_path in file_generator:\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        with open(file_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            text = file.read().strip()\n",
    "            words = text.split(' ')\n",
    "            for word in words:\n",
    "                cf_dict[word] += 1\n",
    "    with open(cf_path, 'wb') as cf_file:\n",
    "        pickle.dump(cf_dict, cf_file)        \n",
    "        \n",
    "base_path = './data/content/'\n",
    "base_result_path = './data/statistics/'\n",
    "\n",
    "text_cf_path = base_result_path + 'text_cf_count.pkz'\n",
    "title_cf_path = base_result_path + 'title_cf_count.pkz'\n",
    "\n",
    "count_cf(get_text_stem_files(base_path), text_cf_path)\n",
    "count_cf(get_title_stem_files(base_path), title_cf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113\n",
      "38113"
     ]
    }
   ],
   "source": [
    "base_path = './data/content/'\n",
    "p_n_path = './data/path.numerate.pkz'\n",
    "text_doclen_path = './data/statistics/text_doclen.pkz'\n",
    "title_doclen_path = './data/statistics/title_doclen.pkz'\n",
    "\n",
    "with open(p_n_path, 'rb') as p_n_file:\n",
    "    path_to_num = pickle.load(p_n_file)\n",
    "    num_to_text_len = defaultdict(int)\n",
    "    num_to_title_len = defaultdict(int)\n",
    "\n",
    "    cnt = 0\n",
    "    for path in get_text_stem_files(base_path):\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        base = get_base_file_path(path)\n",
    "        with open(path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            num_to_text_len[int(path_to_num[base])] = len(file.read().strip().split(' '))\n",
    "\n",
    "    with open(text_doclen_path, 'wb') as text_doclen_file:\n",
    "        pickle.dump(num_to_text_len, text_doclen_file)\n",
    "\n",
    "    print()\n",
    "\n",
    "    cnt = 0\n",
    "    for path in get_title_stem_files(base_path):\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        base = get_base_file_path(path)\n",
    "        with open(path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            num_to_title_len[int(path_to_num[base])] = len(file.read().strip().split(' '))\n",
    "\n",
    "    with open(title_doclen_path, 'wb') as title_doclen_file:\n",
    "        pickle.dump(num_to_title_len, title_doclen_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113\n"
     ]
    }
   ],
   "source": [
    "def count_pairs(file_generator):\n",
    "    cnt = 0\n",
    "    for file_path in file_generator:\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        pairs_dict = defaultdict(int)\n",
    "        with open(file_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            text = file.read().strip()\n",
    "            words = text.split(' ')\n",
    "            for idx in range(len(words) - 1):\n",
    "                pairs_dict[(words[idx], words[idx + 1])] += 1\n",
    "\n",
    "        pairs_path = file_path_to_pairs_path(file_path)\n",
    "        os.makedirs(os.path.dirname(pairs_path), exist_ok=True)\n",
    "        with open(pairs_path, 'wb') as pairs_file:\n",
    "            pickle.dump(pairs_dict, pairs_file)\n",
    "\n",
    "\n",
    "base_path = './data/content/'\n",
    "base_result_path = './data/statistics/'\n",
    "\n",
    "count_pairs(get_text_stem_files(base_path))\n",
    "count_pairs(get_title_stem_files(base_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113\n"
     ]
    }
   ],
   "source": [
    "def count_hop_pairs(file_generator):\n",
    "    cnt = 0\n",
    "    for file_path in file_generator:\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        hop_pairs_dict = defaultdict(int)\n",
    "        with open(file_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            text = file.read().strip()\n",
    "            words = text.split(' ')\n",
    "            for idx in range(len(words) - 2):\n",
    "                hop_pairs_dict[(words[idx], words[idx + 2])] += 1\n",
    "\n",
    "        hop_pairs_path = file_path_to_hop_pairs_path(file_path)\n",
    "        os.makedirs(os.path.dirname(hop_pairs_path), exist_ok=True)\n",
    "        with open(hop_pairs_path, 'wb') as pairs_file:\n",
    "            pickle.dump(hop_pairs_dict, pairs_file)\n",
    "\n",
    "\n",
    "base_path = './data/content/'\n",
    "base_result_path = './data/statistics/'\n",
    "\n",
    "count_hop_pairs(get_text_stem_files(base_path))\n",
    "count_hop_pairs(get_title_stem_files(base_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38113"
     ]
    }
   ],
   "source": [
    "def count_hop_hop_pairs(file_generator):\n",
    "    cnt = 0\n",
    "    for file_path in file_generator:\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        cnt += 1\n",
    "        hop_hop_pairs_dict = defaultdict(int)\n",
    "        with open(file_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "            text = file.read().strip()\n",
    "            words = text.split(' ')\n",
    "            for idx in range(len(words) - 3):\n",
    "                hop_hop_pairs_dict[(words[idx], words[idx + 3])] += 1\n",
    "\n",
    "        hop_hop_pairs_path = file_path_to_hop_hop_pairs_path(file_path)\n",
    "        os.makedirs(os.path.dirname(hop_hop_pairs_path), exist_ok=True)\n",
    "        with open(hop_hop_pairs_path, 'wb') as pairs_file:\n",
    "            pickle.dump(hop_hop_pairs_dict, pairs_file)\n",
    "\n",
    "\n",
    "base_path = './data/content/'\n",
    "base_result_path = './data/statistics/'\n",
    "\n",
    "count_hop_hop_pairs(get_text_stem_files(base_path))\n",
    "count_hop_hop_pairs(get_title_stem_files(base_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to implement an assessor evaluation\n",
    "# (the result hasn't been improved)\n",
    "\n",
    "sample_path = './data/sample.technosphere.ir1.textrelevance.submission.txt'\n",
    "base_path = './data/path.numerate.sort.txt'\n",
    "base_result_path = './data/statistics/b25.similarity.pkz'\n",
    "N = 400\n",
    "\n",
    "def query_to_doc_order(base):\n",
    "    current_id = 1\n",
    "    min = 1\n",
    "    max = 1\n",
    "    query_docs = defaultdict(list)\n",
    "    docs_query = defaultdict(list)\n",
    "\n",
    "    for i in range(1, N):\n",
    "        query_docs[i] = [[], []]\n",
    "\n",
    "    with open(base, 'r', encoding='UTF-8') as file:\n",
    "        file.readline()\n",
    "        for line in file:\n",
    "            query_id, doc_id = map(int, (part.strip() for part in line.split(',')))\n",
    "            docs_query[doc_id].append(query_id)\n",
    "            if query_id == current_id:\n",
    "                if min <= doc_id <= max:\n",
    "                    max += 1\n",
    "                else:\n",
    "                    query_docs[query_id][1].append(doc_id)\n",
    "            else:\n",
    "                query_docs[current_id][0] = [min, max - 1]\n",
    "                current_id = query_id\n",
    "                min = max\n",
    "                if min <= doc_id <= max:\n",
    "                    max += 1\n",
    "                else:\n",
    "                    query_docs[query_id][1].append(doc_id)\n",
    "        query_docs[query_id][0] = [min, max - 1]\n",
    "    return query_docs, docs_query\n",
    "\n",
    "def path_to_text(base, docs):\n",
    "    texts = list()\n",
    "    with open(base, 'r', encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            path, doc_id = (part.strip() for part in line.split('\\t'))\n",
    "            doc_id = int(doc_id)\n",
    "            if doc_id in docs:\n",
    "                tokens = path.split('/')\n",
    "                text_path = path.replace(tokens[3], tokens[3] + '_text_stem')\n",
    "                title_path = path.replace(tokens[3], tokens[3] + '_title_stem')\n",
    "                with open(text_path, 'r', encoding='UTF-8') as text, \\\n",
    "                    open(title_path, 'r', encoding='UTF-8') as title:\n",
    "                    t1 = title.read().strip()\n",
    "                    t2 = text.read().strip()\n",
    "                    texts.append((t1 + ' ' + t2).split())\n",
    "    return texts\n",
    "\n",
    "def py_bm25_sim(base, dest, sample):\n",
    "    cnt = 0\n",
    "    query_docs, docs_query = query_to_doc_order(sample)\n",
    "    query_docs_sim = defaultdict(list)\n",
    "\n",
    "    for i in range(1, N):\n",
    "        cnt += 1\n",
    "        print('\\r{}'.format(cnt), end='')\n",
    "        texts = path_to_text(base, list(range(query_docs[i][0][0], query_docs[i][0][1])) + query_docs[i][1])\n",
    "        result = get_bm25_weights(texts, 4)\n",
    "        query_docs_sim[i] = result\n",
    "\n",
    "    with open(dest, 'wb', encoding='UTF-8') as docs_sim_file:\n",
    "        pickle.dump(query_docs_sim, docs_sim_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 16.599 0 16.93 0\n",
      "4 28658 7.591 0 5.829 0\n",
      "2 3 192159505 37.045 0 77.388 0\n",
      " 13.244 0 10.629 0\n",
      "1 done\n",
      "4 done\n",
      "2 done\n",
      "3 done\n"
     ]
    }
   ],
   "source": [
    "def get_idf(word):\n",
    "    cf = text_cf_dict[word] + title_cf_dict[word] + 1\n",
    "    df = text_df_dict[word] + title_df_dict[word] + 1\n",
    "    idf_1 = -math.log(1 - math.exp(-1.5 * cf / N_DOCUMENTS)) - math.log(df / N_DOCUMENTS)\n",
    "    idf_2 = math.log(TOTAL_LEMMS / cf)\n",
    "    return (idf_1 + idf_2) / 2\n",
    "\n",
    "\n",
    "def get_score(query, doc_id, doc_base_path):\n",
    "    if len(query) == 0:\n",
    "        return 0\n",
    "\n",
    "    text_tf_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/tf').replace('/doc.', '_text_stem/doc.').replace('.dat', '.pkz')\n",
    "    title_tf_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/tf').replace('/doc.', '_title_stem/doc.').replace('.dat', '.pkz')\n",
    "\n",
    "    text_pairs_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/pairs').replace('/doc.', '_text_stem/doc.').replace('.dat', '.pkz')\n",
    "    title_pairs_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/pairs').replace('/doc.', '_title_stem/doc.').replace('.dat', '.pkz')\n",
    "\n",
    "    text_hop_pairs_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/hop_pairs').replace('/doc.', '_text_stem/doc.').replace('.dat', '.pkz')\n",
    "    title_hop_pairs_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/hop_pairs').replace('/doc.', '_title_stem/doc.').replace('.dat', '.pkz')\n",
    "\n",
    "    text_hop_hop_pairs_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/hop_hop_pairs').replace('/doc.', '_text_stem/doc.').replace('.dat', '.pkz')\n",
    "    title_hop_hop_pairs_path = doc_base_path \\\n",
    "        .replace('content', 'statistics/hop_hop_pairs').replace('/doc.', '_title_stem/doc.').replace('.dat', '.pkz')\n",
    "\n",
    "    score = 0\n",
    "    with open(text_tf_path, 'rb') as text_tf_file, \\\n",
    "            open(title_tf_path, 'rb') as title_tf_file, \\\n",
    "            open(text_pairs_path, 'rb') as text_pairs_file, \\\n",
    "            open(title_pairs_path, 'rb') as title_pairs_file, \\\n",
    "            open(text_hop_pairs_path, 'rb') as text_hop_pairs_file, \\\n",
    "            open(title_hop_pairs_path, 'rb') as title_hop_pairs_file, \\\n",
    "            open(text_hop_hop_pairs_path, 'rb') as text_hop_hop_pairs_file, \\\n",
    "            open(title_hop_hop_pairs_path, 'rb') as title_hop_hop_pairs_file:\n",
    "\n",
    "        text_tf_dict = pickle.load(text_tf_file)\n",
    "        title_tf_dict = pickle.load(title_tf_file)\n",
    "\n",
    "        text_pairs_dict = pickle.load(text_pairs_file)\n",
    "        title_pairs_dict = pickle.load(title_pairs_file)\n",
    "\n",
    "        text_hop_pairs_dict = pickle.load(text_hop_pairs_file)\n",
    "        title_hop_pairs_dict = pickle.load(title_hop_pairs_file)\n",
    "\n",
    "        text_hop_hop_pairs_dict = pickle.load(text_hop_hop_pairs_file)\n",
    "        title_hop_hop_pairs_dict = pickle.load(title_hop_hop_pairs_file)\n",
    "\n",
    "        words = query.split(' ')\n",
    "        W_single = 0\n",
    "        W_pair = 0\n",
    "        W_all = 0\n",
    "\n",
    "        doclen = float(text_doclen_dict[doc_id] + title_doclen_dict[doc_id])\n",
    "        N_missed = 0\n",
    "        sum_log_p = 0\n",
    "\n",
    "        for word in words:\n",
    "            tf = text_tf_dict[word] + title_tf_dict[word]\n",
    "            hdr = title_tf_dict[word]\n",
    "\n",
    "            IDF = get_idf(word)\n",
    "\n",
    "            TF_1 = tf / (tf + K1 + K2 * doclen)\n",
    "            TF_2 = 1 if hdr > 0 else 0\n",
    "\n",
    "            W_single += IDF * (TF_1 + 1.5 * TF_2)\n",
    "\n",
    "            sum_log_p += IDF\n",
    "            if tf < 1:\n",
    "                N_missed += 1\n",
    "\n",
    "        for word_1, word_2 in combinations(words, 2):\n",
    "            IDF_1 = get_idf(word_1)\n",
    "            IDF_2 = get_idf(word_2)\n",
    "\n",
    "            pair_1_2 = text_pairs_dict[(word_1, word_2)] + title_pairs_dict[(word_1, word_2)]\n",
    "            pair_2_1 = text_pairs_dict[(word_2, word_1)] + title_pairs_dict[(word_2, word_1)]\n",
    "            hop_pair_1_2 = text_hop_pairs_dict[(word_1, word_2)] + title_hop_pairs_dict[(word_1, word_2)]\n",
    "            hop_hop_pair_1_2 = text_hop_hop_pairs_dict[(word_1, word_2)] + title_hop_hop_pairs_dict[(word_1, word_2)]\n",
    "\n",
    "            TF = (W_P * pair_1_2 + W_S * hop_pair_1_2 + W_W * hop_hop_pair_1_2 + W_I * pair_2_1)\n",
    "            W_pair += (IDF_1 + IDF_2) * TF / (1 + TF)\n",
    "\n",
    "        W_all = sum_log_p * (0.03 ** N_missed)\n",
    "\n",
    "        score = W_single + W_PAIR * W_pair + W_ALL * W_all\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "n_p_path = './data/numerate.path.pkz'\n",
    "\n",
    "target_docs_path = './data/sample.technosphere.ir1.textrelevance.submission.txt'\n",
    "\n",
    "text_doclen_path = './data/statistics/text_doclen.pkz'\n",
    "title_doclen_path = './data/statistics/title_doclen.pkz'\n",
    "text_cf_path = './data/statistics/text_cf_count.pkz'\n",
    "title_cf_path = './data/statistics/title_cf_count.pkz'\n",
    "text_df_path = './data/statistics/text_df_count.pkz'\n",
    "title_df_path = './data/statistics/title_df_count.pkz'\n",
    "\n",
    "with open(text_cf_path, 'rb') as text_cf_file, \\\n",
    "        open(title_cf_path, 'rb') as title_cf_file, \\\n",
    "        open(text_df_path, 'rb') as text_df_file, \\\n",
    "        open(title_df_path, 'rb') as title_df_file, \\\n",
    "        open(text_doclen_path, 'rb') as text_doclen_file, \\\n",
    "        open(title_doclen_path, 'rb') as title_doclen_file:\n",
    "\n",
    "    text_cf_dict = pickle.load(text_cf_file)\n",
    "    title_cf_dict = pickle.load(title_cf_file)\n",
    "    text_df_dict = pickle.load(text_df_file)\n",
    "    title_df_dict = pickle.load(title_df_file)\n",
    "    text_doclen_dict = pickle.load(text_doclen_file)\n",
    "    title_doclen_dict = pickle.load(title_doclen_file)\n",
    "\n",
    "\n",
    "def process(part_idx):\n",
    "    queries_path = './data/queries.numerate_processed_chunks/chunk_{0}.txt'.format(part_idx)\n",
    "    submition_path = './data/queries.numerate_processed_chunks/results/chunk_{0}.txt'.format(part_idx)\n",
    "\n",
    "    queries = defaultdict(tuple)\n",
    "    with open(queries_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            idx, query_main, query_ex, query_trans_main, query_trans_ex = (part.strip() for part in line.split('\\t'))\n",
    "            queries[int(idx)] = (query_main, query_ex, query_trans_main, query_trans_ex)\n",
    "\n",
    "    with open(n_p_path, 'rb') as file:\n",
    "        n_p_dict = pickle.load(file)\n",
    "\n",
    "    result = defaultdict(list)\n",
    "    queries_processed = 0\n",
    "    checked = False\n",
    "    with open(target_docs_path, 'r', encoding='UTF-8', errors='ignore') as file:\n",
    "        file.readline()\n",
    "        for line in file:\n",
    "            query_id, doc_id = (int(val) for val in line.strip().split(','))\n",
    "            if query_id not in queries:\n",
    "                continue\n",
    "            try:\n",
    "                query_main, query_ex, query_trans_main, query_trans_ex = queries[query_id]\n",
    "                score_main = get_score(query_main, doc_id, n_p_dict[doc_id])\n",
    "                score_ex = get_score(query_ex, doc_id, n_p_dict[doc_id])\n",
    "                score_trans_main = get_score(query_trans_main, doc_id, n_p_dict[doc_id])\n",
    "                score_trans_ex = get_score(query_trans_ex, doc_id, n_p_dict[doc_id])\n",
    "                score = max(score_main, score_ex, score_trans_main, score_trans_ex)\n",
    "                if not checked:\n",
    "                    print(part_idx,\n",
    "                          doc_id,\n",
    "                          np.round(score_main, 3),\n",
    "                          np.round(score_ex, 3),\n",
    "                          np.round(score_trans_main, 3),\n",
    "                          np.round(score_trans_ex, 3))\n",
    "                    checked = True\n",
    "            except KeyError:\n",
    "                score = 0\n",
    "            result[query_id].append((doc_id, score))\n",
    "            queries_processed += 1\n",
    "\n",
    "    for key in result.keys():\n",
    "        result[key] = sorted(result[key], key=lambda val: val[1], reverse=True)\n",
    "\n",
    "    with open(submition_path, 'w') as file:\n",
    "        file.write('QueryId,DocumentId\\n')\n",
    "        for key in sorted(result.keys()):\n",
    "            for doc_id, _ in result[key]:\n",
    "                file.write(str(key) + ',' + str(doc_id) + '\\n')\n",
    "    print(part_idx, 'done')\n",
    "\n",
    "\n",
    "pool = multiprocessing.pool.ThreadPool(4)\n",
    "pool.map(process, list(range(1, 5)))\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './data/queries.numerate_processed_chunks/results/'\n",
    "submition_path = base_path + 'out.submition_{0}.txt'.format(str(time.time()))\n",
    "\n",
    "paths = sorted([f for f in os.listdir(base_path) if f.find('chunk') >= 0])\n",
    "\n",
    "with open(submition_path, 'w') as submition_file:\n",
    "    submition_file.write('QueryId,DocumentId\\n')\n",
    "    for path in paths:\n",
    "        with open(base_path + path, 'r') as file:\n",
    "            file.readline()\n",
    "            for line in file:\n",
    "                submition_file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
